---
layout:     post
title:      "Деревья принятия решений в R"
date:       2015-11-10 22:00:00
author:     "Vasiliy Zemlyanov"
header-img: "img/post-bg-03.jpg"
---

##TODO:
```
Использовать для статьи пост о титанике
```

Дерево принятия решений - средство поддержки принятия решений, использующееся в статистике и анализе данных для прогнозных моделей. Структура дерева представляет собой «листья» и «ветки».

Достоинством деревьев является возможность интерпритировать результат.

#Модель
Данная модель строит дерево на основе разбияния независимых переменных.

##Картинка с примером дерева

##Как выбрать количество ветвлений
Один из сопсобов определить количество разбиений - задать `minBucket` параметр, минимальное количестов элементов которое должно попасть в каждое ветвление.
Если `minBucket` будет слишком большим:
Если `minBucket` будет слишком низким:

##ROC кривая

{% highlight R %}
# Backup source from R
setwd("F:/Dev/analitics/trees/")
stevens = read.csv("stevens.csv")

str(stevens)
set.seed(3000)

library(caTools)
split = sample.split(stevens$Revers, SplitRatio = 0.7)
Train = subset(stevens, split == TRUE)
Test = subset(stevens, split == FALSE)

#  Installing package 
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

#  Building model
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent +
                      LowerCourt + Unconst, data = Train, method = "class",
                    control=rpart.control(minbucket = 25))

#  Plot tree
prp(StevensTree)

# Predicitons
PredictCART = predict(StevensTree, newdata = Test, type="class")

# Compute accurasy
table(Test$Reverse, PredictCART)

# Plot ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
pref = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pref, "tpr", "fpr")
plot(perf)
{% endhighlight %}

#Random forest
Этот метод был разработан для улучшения предсказывающих качеств деревьев принятия решений и работает на основе построение большого количества деревьев.
Ценой за улучшение предсказывающих способностей является уменьшение интерпритируемости результатов.

Для того, чтобы сделать предсказание с помощью `Random forest` каждое дерево *голосует* за результат и мы выбираем результат модели по большинству голосов.

Каждое дерево строится из `badged or bootstrapped` подмножества данных с повторениями.

Параметры Random Forest:

- Минимальное количество наблюдений в подмножестве (`nodesize`)
- Количество деревьев (`ntree`)

## Random forest в R

{% highlight R %}
### Random forest example

# install package
install.packages("randomForest")
library(randomForest)

# Building model
# Outcome variable should be a factor
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)

# nodesize - number of items in bucket; ntree - number of trees to build
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst,
                             data=Train, nodesize=25, ntree=200)

# Predicitons
PredictForest = predict(StevensForest, newdata = Test)

# Confusion matrix
table(Test$Reverse, PredictForest)
(40 + 75) / (40 + 37 + 18 + 75)
{% endhighlight %}